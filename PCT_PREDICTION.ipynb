{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import shap\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Dictionary to store uploaded files\n",
        "uploaded_files = {}\n",
        "\n",
        "# Prompt for and upload multiple files\n",
        "num_files = int(input(\"How many files do you want to upload? \"))\n",
        "\n",
        "for i in range(num_files):\n",
        "    print(f\"Upload file {i+1}:\")\n",
        "    uploaded = files.upload()\n",
        "    filename = next(iter(uploaded))  # Get the filename\n",
        "    uploaded_files[filename] = uploaded[filename]  # Store file data\n",
        "\n",
        "# Ensure at least two files are uploaded\n",
        "if len(uploaded_files) < 2:\n",
        "    raise ValueError(\"Error: At least two files are required.\")\n",
        "\n",
        "# Get dataset and new patient data filenames\n",
        "dataset_filename, new_patient_filename = list(uploaded_files.keys())[:2]\n",
        "\n",
        "# Read the dataset and new patient data files\n",
        "dataset = pd.read_excel(dataset_filename)\n",
        "new_patient_data = pd.read_excel(new_patient_filename)\n",
        "\n",
        "print(\"Dataset and new patient data loaded successfully.\")\n",
        "\n",
        "# Display dataset information\n",
        "print(dataset.head(10))\n",
        "print(dataset.info())\n",
        "\n",
        "# Calculate min, max, median, and 25th-75th percentiles\n",
        "summary = dataset.describe(percentiles=[0.25, 0.5, 0.75]).loc[['min', '25%', '50%', '75%', 'max']]\n",
        "summary = summary.rename(index={\n",
        "    'min': 'Minimum',\n",
        "    '50%': 'Median',\n",
        "    '25%': '25th Percentile',\n",
        "    '75%': '75th Percentile',\n",
        "    'max': 'Maximum'\n",
        "})\n",
        "\n",
        "# Save summary statistics to Excel\n",
        "summary_filename = 'summary_statistics.xlsx'\n",
        "summary.to_excel(summary_filename, sheet_name='Summary')\n",
        "print(f\"Summary statistics saved to {summary_filename}\")\n",
        "\n",
        "# Display the summary as a table\n",
        "print(tabulate(summary, headers='keys', tablefmt='grid'))\n",
        "\n",
        "# Correlation Matrix Plot\n",
        "plt.figure(figsize=(25, 14))\n",
        "sns.heatmap(dataset.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', square=True)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Select features and target variable\n",
        "features = [\n",
        "    \"BA%\", \"EO#\", \"EO%\", \"HCT\", \"HGB\", \"IG%\", \"LY#\", \"LY%\", \"MCH\", \"MCHC\", \"MCV\", \"MO#\", \"MO%\", \"MPV\",\n",
        "    \"NE#\", \"NE%\", \"PDW\", \"P-LCR\", \"PLT\", \"RBC\", \"RDW-CV\", \"RDW-SD\", \"WBC\", \"Creatinine\", \"CRP\", \"NE#/LY#\", \"ScrxCRP\", \"NE#xCRP\", \"P-LCRxCRP\"\n",
        "]\n",
        "target = \"Procalcitonin\"\n",
        "\n",
        "# Handle missing values\n",
        "data = dataset.fillna(dataset.median(skipna=True))\n",
        "\n",
        "# Create a binary target variable for procalcitonin ≥ 0.5\n",
        "data['Procalcitonin_Binary'] = (data[target] >= 0.5).astype(int)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[features], data['Procalcitonin_Binary'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute class weight ratio\n",
        "class_weight_ratio = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
        "\n",
        "# Define hyperparameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 200, 300, 400, 500],\n",
        "    'max_depth': [3, 4, 5, 6, 7, 9],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [1, 1.5, 2],\n",
        "    'scale_pos_weight': [1, len(y_train[y_train == 0]) / len(y_train[y_train == 1])]\n",
        "}\n",
        "\n",
        "\n",
        "# Initialize and perform RandomizedSearchCV\n",
        "model = XGBClassifier(random_state=42)\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, scoring='accuracy', cv=10, random_state=42, n_jobs=-1)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve best parameters and best estimator\n",
        "best_params = random_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Get cross-validation results and display top results\n",
        "cv_results_df = pd.DataFrame(random_search.cv_results_)\n",
        "top_results = cv_results_df.sort_values(by='mean_test_score', ascending=False)\n",
        "print(top_results[['params', 'mean_test_score', 'std_test_score']].head())\n",
        "\n",
        "# Feature Importance Plot\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': best_model.feature_importances_})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df['Feature'], palette=\"viridis\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importances For Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "# Make predictions\n",
        "predictions = best_model.predict(X_test)\n",
        "prediction_probs = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "class_report = classification_report(y_test, predictions, output_dict=True)\n",
        "\n",
        "# Convert classification report to a tabular format\n",
        "class_report_list = [\n",
        "    ['accuracy', class_report['accuracy'], '', '', '']\n",
        "] + [\n",
        "    [key, value['precision'], value['recall'], value['f1-score'], value['support']]\n",
        "    for key, value in class_report.items() if key != 'accuracy'\n",
        "]\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(tabulate(class_report_list, headers=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"], tablefmt=\"grid\"))\n",
        "\n",
        "# Calculate Positive Predictive Value (PPV) and Negative Predictive Value (NPV)\n",
        "tn, fp, fn, tp = conf_matrix.ravel()\n",
        "ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
        "npv = tn / (tn + fn) if (tn + fn) != 0 else 0\n",
        "\n",
        "print(f'Positive Predictive Value (PPV): {ppv:.2f}')\n",
        "print(f'Negative Predictive Value (NPV): {npv:.2f}')\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(conf_matrix, cmap='Blues', interpolation='nearest')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "\n",
        "# Add text annotations for TP, TN, FP, FN\n",
        "for i in range(conf_matrix.shape[0]):\n",
        "    for j in range(conf_matrix.shape[1]):\n",
        "        plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='red', fontsize=16)\n",
        "\n",
        "# Add labels for the axes\n",
        "tick_marks = np.arange(2)\n",
        "plt.xticks(tick_marks, ['< 0.5', '≥ 0.5'])\n",
        "plt.yticks(tick_marks, ['< 0.5', '≥ 0.5'])\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, prediction_probs)\n",
        "roc_auc = roc_auc_score(y_test, prediction_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate sensitivity and specificity\n",
        "sensitivity = tpr\n",
        "specificity = 1 - fpr\n",
        "\n",
        "# Print AUC, sensitivity, and specificity\n",
        "print(f'AUC: {roc_auc:.2f}')\n",
        "print(f'Sensitivity: {sensitivity}')\n",
        "print(f'Specificity: {specificity}')\n",
        "\n",
        "# Feature Importance Plot\n",
        "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': best_model.feature_importances_})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=feature_importance_df['Importance'], y=feature_importance_df['Feature'], palette=\"viridis\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.title(\"Feature Importances For Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "# SHAP Explanation\n",
        "explainer = shap.Explainer(best_model, X_test) # Pass the fitted 'best_model'\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "# SHAP Summary Plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=features)\n",
        "\n",
        "# SHAP Beeswarm Plot\n",
        "shap.plots.beeswarm(shap_values)\n",
        "\n",
        "# Automated SHAP Dependence Plots for the Top 20 Most Important Features\n",
        "top_features = feature_importance_df[\"Feature\"].head(20).tolist()\n",
        "\n",
        "for feature in top_features:\n",
        "    shap.dependence_plot(feature, shap_values.values, X_test, feature_names=features)\n",
        "    plt.show()\n",
        "\n",
        "# Make predictions for new patients\n",
        "new_patient_data_array = new_patient_data[features].to_numpy()\n",
        "# Use the trained model (best_model) instead of the untrained model (model)\n",
        "predicted_procalcitonin_binary = best_model.predict(new_patient_data_array)\n",
        "print(\"Predicted procalcitonin binary for new patients:\")\n",
        "print(predicted_procalcitonin_binary)\n",
        "\n",
        "# Save the predictions to an Excel file\n",
        "output_file_path = 'new_patient_predictions.xlsx'\n",
        "new_patient_data['Predicted_Procalcitonin_Binary'] = predicted_procalcitonin_binary\n",
        "new_patient_data.to_excel(output_file_path, index=False)\n",
        "print(f\"Predictions saved to {output_file_path}\")\n",
        "\n",
        "# Step 8: Download Results\n",
        "files.download('new_patient_predictions.xlsx')"
      ],
      "metadata": {
        "id": "8zkfxAzYEWrt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}